{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents \n",
    "\n",
    "1. [Introduction to the task](#1.-Introduction-to-the-task)\n",
    "\n",
    "2. [Dataset](#2.-Datasets)\n",
    "    \n",
    "    2.1 [Datasets format](#2.1-Datasets-format)\n",
    "\n",
    "3. [Model architecture](#3.-Model-architecture)\n",
    "\n",
    "4. [Get started with the model](#4.-Get-started-with-the-model)\n",
    "    \n",
    "    4.1 [Installation](#4.1-Installation)\n",
    "\n",
    "    4.2 [Support dataset configuration](#4.2-Support-dataset-configuration)\n",
    "\n",
    "5. [Use the model for prediction](#5.-Use-the-model-for-prediction)\n",
    "\n",
    "    5.1 [Predict using Python](#5.1-Predict-using-Python)\n",
    "    \n",
    "    5.2 [Predict using CLI](#5.2-Predict-using-CLI)\n",
    "     \n",
    "6. [Train the model on your data](#6.-Train-the-model-on-your-data)\n",
    "    \n",
    "    6.1. [Train your model from Python](#6.1-Train-your-model-from-Python)\n",
    "    \n",
    "    6.2. [Train your model from CLI](#6.2-Train-your-model-from-CLI)\n",
    "    \n",
    "7. [Evaluate](#7.-Evaluate)\n",
    "    \n",
    "    7.1. [Evaluate from Python](#7.1-Evaluate-from-Python)\n",
    "    \n",
    "    7.2. [Evaluate from CLI](#7.2-Evaluate-from-CLI)\n",
    "    \n",
    "8. [Metrics](#8.-Metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction to the task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Intent classification__ is a task of identifying speaker's intent given an utterance, where intent is one of N classes or \"OOS\" (out-of-scope examples - utterances that do not belong to any of the predefined classes). We consider few-shot setting, where only few examples (5 or 10) per intent class are given as a training set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our experiments we used the [CLINC150](https://paperswithcode.com/dataset/clinc150) dataset, which has 10 different domains with 15 intents each, 100 shots per intent class and 1000 OOS examples. It simulates a setting, where model has to handle many different services with wide variety of intents.\n",
    "\n",
    "Specifically, we validate our model on CLINC150 from the original DNNC paper. We parsed it to match the format described [below](#21-dataset-format). The original dataset can be downloaded from the DNNC [github page](https://github.com/salesforce/DNNC-few-shot-intent)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Datasets format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train, dev and test set are separate json files, which have the following format\n",
    "\n",
    "```\n",
    "{\n",
    "    \"columns\": [\n",
    "        \"text\",\n",
    "        \"category\"\n",
    "    ],\n",
    "\n",
    "    \"data\": [\n",
    "\n",
    "        [\n",
    "            \"text\"\n",
    "            \"intent_class\"\n",
    "        ],\n",
    "\n",
    "        ...\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The typical methodology of few-shot intent classification is to embed each example into a vector space and use an off-the-shelf distance metric to perform a similarity search. However, the text embedding methods do not discriminate the OOS examples well enough.\n",
    "\n",
    "\n",
    "DNNC authors suggests to model fine-grained relations of utterance pairs via pairise simmilarity:\n",
    "\n",
    "$h = BERT([[CLS], u, [SEP], e_{j,i}, [SEP]]) \\in \\R^d$\n",
    "\n",
    "$S(u, e_{j,i}) = \\sigma(W * h + b) \\in \\R$, where $e_{j, i} \\in E $- training set, $W \\in \\R^{1×d}$, $b \\in \\R$\n",
    "\n",
    "To mitigate the data scarcity setting in few-shot learning, DNNC uses knowldge-transfer from NLI task. We pretrain [roberta-base](https://huggingface.co/roberta-base) on combination of 3 NLI datasets: SNLI, WNLI, MNLI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Get started with the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First make sure you have the DeepPavlov Library installed.\n",
    "[More info about the first installation.](http://docs.deeppavlov.ai/en/master/intro/installation.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install deeppavlov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then make sure that all the required packages for the model are installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m deeppavlov install dnnc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dnnc` is the name of the model's *config_file*. [What is a Config File?](http://docs.deeppavlov.ai/en/master/intro/configuration.html) \n",
    "\n",
    "Configuration file defines the model and describes it's hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Support dataset configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before making predictions or evaluation you need to set path to your support dataset. DNNC model compares input text to every example in support dataset to determine, which class the input example belongs to. By default, the model uses training data as support dataset. You can specify the support dataset path in the in `dnnc` config file. It has the same format as metioned [before]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.core.commands.utils import parse_config\n",
    "\n",
    "model_config = parse_config('dnnc')\n",
    "\n",
    "#  dataset for predictions\n",
    "print(model_config['chainer']['pipe'][0]['support_dataset_path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Off-the-shelf prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base model was already pre-trained to recognize simmilar utterances, so you can use off-the-shelf model to make predictions and evalutation. No additional training needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OOS prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out-of-scope (OOS) examples are determined via confidence_threshold parameter with the following algorithm. Firstly model calculates an average similarity score for every intent class from support dataset. Secondly it determines the class with maximum similarity score. Finally the model predicts class with maximum similarity if it's score is higher than confidence_threshold and \"oos\" class otherwise. The higher the threshold, the more often the model predicts \"oos\" class. By default it is set to 0.5. You can change it to your preferences in configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.core.commands.utils import parse_config\n",
    "\n",
    "model_config = parse_config('dnnc')\n",
    "\n",
    "#  dataset for predictions\n",
    "print(model_config['chainer']['pipe'][-1]['confidence_threshold'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Use the model for prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Predict using Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After [installing](#4.-Get-started-with-the-model) the model, build it from the config and predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov import build_model, configs\n",
    "\n",
    "model = build_model(\"dnnc\", download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['book_hotel', 'international_visa']\n"
     ]
    }
   ],
   "source": [
    "model([\"can you find me a good reviewed hotel in japan\", \"if i get a visa can i travel to japan\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Predict using CLI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also get predictions in an interactive mode through CLI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m deeppavlov interact dnnc -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or make predictions for samples from *stdin*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m deeppavlov predict dnnc -f <file-name>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Train the model on your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a separate `roberta_nli` config for training, which automatically transforms dataset into a pairwise format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model on your data, you need to change the path to the dataset in `roberta_nli` config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.core.commands.utils import parse_config\n",
    "\n",
    "model_config = parse_config('roberta_nli')\n",
    "\n",
    "#  dataset for training\n",
    "print(model_config['dataset_reader']['data_path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Train your model from Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov import train_model\n",
    "\n",
    "model = train_model(\"roberta_nli\", download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Train your model from CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m deeppavlov train roberta_nli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the model on your data, you need to change the path to the dataset in `dnnc` config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.core.commands.utils import parse_config\n",
    "\n",
    "model_config = parse_config('dnnc')\n",
    "\n",
    "#  dataset for evaluation\n",
    "print(model_config['dataset_reader']['data_path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Evaluate from Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov import evaluate_model\n",
    "\n",
    "model = evaluate_model('dnnc', download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Evaluate from CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m deeppavlov evaluate dnnc -d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('dnnc_cuda11_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d267cc100fe706c63aefa7fd2da1b610b862ac822a3924399ae410740f5c813e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
